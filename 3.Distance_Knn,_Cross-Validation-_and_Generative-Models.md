Distance, Knn, Cross Validation, and Generative Models
================
PK (Kasidit) Ratanavijai
07/13/2019

## Distance

The concept of distance is quite intuitive. For example, when we cluster
animals into subgroups, reptiles, amphibians, mammals, we’re implicitly
defining a **distance** that permits us to say what animals are close to
each other.

Many machine learning techniques rely on being able to define distance
between observations using features or predictors.

We can also compute all the distances between all the observations at
once relatively quickly using the function, dist.

An interesting thing to note is that, if we pick a predictor, a pixel,
we can see which pixels are close, meaning that they are either ink
together or they don’t have ink together by observe distance value

## Knn

K-nearest neighbors(Knn) is similar to bin smoothing. But it is easier
to adapt to multiple dimensions. We first defined the distance between
observations based on the features. Basically, for any point for which
you want to estimate the conditional probability, we look at the
k-nearest points and then take an average of these points. Knn give us
the estimated conditional probability. Larger Ks result in smaller
estimates, while smaller Ks result in more flexible and more wiggly
estimates.

We bring up logistic regression again to compare its accuracy with
knn

``` r
fit_glm = glm (y ~ x_1 + x_2 , data =mnist_27$train , family = "binomial")
p_hat_logis = predict (fit_glm, mnist_27$train)
y_hat_logis = factor(ifelse(p_hat_logis >0.5 , 7 ,2))
confusionMatrix(data = y_hat_logis , reference = mnist_27$train$y)$overall[1]
```

    ## Accuracy 
    ##  0.78375

Lets comparethis with knn

``` r
#where . mean all predictor x_1 and x_2
knn_fit = knn3(y ~., data = mnist_27$train , k =5)

y_hat_knn = predict(knn_fit, mnist_27$test, type = "class")

confusionMatrix(data = y_hat_knn, reference =mnist_27$test$y)
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  2  7
    ##          2 91 22
    ##          7 15 72
    ##                                           
    ##                Accuracy : 0.815           
    ##                  95% CI : (0.7541, 0.8663)
    ##     No Information Rate : 0.53            
    ##     P-Value [Acc > NIR] : <2e-16          
    ##                                           
    ##                   Kappa : 0.6271          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.3239          
    ##                                           
    ##             Sensitivity : 0.8585          
    ##             Specificity : 0.7660          
    ##          Pos Pred Value : 0.8053          
    ##          Neg Pred Value : 0.8276          
    ##              Prevalence : 0.5300          
    ##          Detection Rate : 0.4550          
    ##    Detection Prevalence : 0.5650          
    ##       Balanced Accuracy : 0.8122          
    ##                                           
    ##        'Positive' Class : 2               
    ## 

And we see that we already have an improvement over the logistic
regression. Our accuracy is 0.815.

### Overtraining and Oversmoothing

You can see that the accuracy computed on the training side is quite
higher. It’s 0.882 compared to what we get on the test set, which is
only 0.815. This is because we overtrained.

**Overtraining** is at its worst when we set k equals to 1. With k
equals to 1, the estimate for each point in the training set is obtained
with just the y corresponding to that point because you are your closest
neighbor. However, when we check on the test set, the accuracy is
actually worse than with logistic regression. It’s only 0.74.

``` r
#where . mean all predictor x_1 and x_2
knn_fit = knn3(y ~., data = mnist_27$train , k =1)

y_hat_knn = predict(knn_fit, mnist_27$test, type = "class")

confusionMatrix(data = y_hat_knn, reference =mnist_27$test$y)
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  2  7
    ##          2 84 30
    ##          7 22 64
    ##                                           
    ##                Accuracy : 0.74            
    ##                  95% CI : (0.6734, 0.7993)
    ##     No Information Rate : 0.53            
    ##     P-Value [Acc > NIR] : 8.656e-10       
    ##                                           
    ##                   Kappa : 0.4756          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.3317          
    ##                                           
    ##             Sensitivity : 0.7925          
    ##             Specificity : 0.6809          
    ##          Pos Pred Value : 0.7368          
    ##          Neg Pred Value : 0.7442          
    ##              Prevalence : 0.5300          
    ##          Detection Rate : 0.4200          
    ##    Detection Prevalence : 0.5700          
    ##       Balanced Accuracy : 0.7367          
    ##                                           
    ##        'Positive' Class : 2               
    ## 

Let’s try a much larger K.

``` r
#where . mean all predictor x_1 and x_2
knn_fit = knn3(y ~., data = mnist_27$train , k =401)

y_hat_knn = predict(knn_fit, mnist_27$test, type = "class")

confusionMatrix(data = y_hat_knn, reference =mnist_27$test$y)
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction  2  7
    ##          2 92 28
    ##          7 14 66
    ##                                           
    ##                Accuracy : 0.79            
    ##                  95% CI : (0.7269, 0.8443)
    ##     No Information Rate : 0.53            
    ##     P-Value [Acc > NIR] : 1.978e-14       
    ##                                           
    ##                   Kappa : 0.5749          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.04486         
    ##                                           
    ##             Sensitivity : 0.8679          
    ##             Specificity : 0.7021          
    ##          Pos Pred Value : 0.7667          
    ##          Neg Pred Value : 0.8250          
    ##              Prevalence : 0.5300          
    ##          Detection Rate : 0.4600          
    ##    Detection Prevalence : 0.6000          
    ##       Balanced Accuracy : 0.7850          
    ##                                           
    ##        'Positive' Class : 2               
    ## 

We see that the accuracy on a test set is only 0.79, not very good.The
size of k is so large that it does not permit enough flexibility. We’re
almost including half the data to compute each single estimated
conditional probability. We call this **oversmoothing**.

### Summary

We can avoid overtraining and oversmoothing picking a middle number by
trial and error.

``` r
ks = seq (3,251,2)

accuracy = map_df
knn_fit = knn3(y ~., data = mnist_27$train , k =5)

y_hat_knn = predict(knn_fit, mnist_27$test, type = "class")
```

## Cross-validation

Cross-validation is a technique that permits us to alleviate both
overtraining and oversmoothing problems.

The general idea for all of them is to randomly generate smaller data
sets that are not used for training and instead are used to estimate the
true error.

So we need to optimize the algorithm parameters lambda without using our
test set. And we know that if we optimize and evaluated on the same data
set, we will overtrain.

This is where cross-pollination is most useful. So let’s describe k-fold
cross-validation. For each set of algorithm parameters being considered,
we want to estimate the MSE. And then we will choose the parameters with
the smallest MSE. Cross-validation will provide this estimate.

In k-fold cross-validation, we randomly split the observations into k
non-overlapping sets. So now we repeat this calculation for each of
these sets, b going from 1 all the way up to k. So we obtain k estimates
of the MSE.

A final step would be to select the lambda, the parameters, that
minimize the MSE.

### Bootstrap

The bootstrap permits us to approximate a Monte Carlo simulation without
access to the entire distribution. The general idea is relatively
simple. We act as if the sample is the entire population and sample with
replacement data sets of the same size. Then we compute the summary
statistic, in this case, the median, on what is called the bootstrap
sample.

## Generative Models

In a binary case, the best we can do is called Bayes’ rule which is a
decision rule based on the true conditional probability, probably y
equals one given the predictors
x.  
![1](https://latex.codecogs.com/gif.latex?%5Cdpi%7B120%7D%20p%28x%29%20%3D%20Pr%28Y%3D1%7CX%3Dx%29)

We have described several approaches to estimating this conditional
probability.

Methods that model the joint distribution of y and the predictors x are
referred to as **generative models**.

### Naive Bayes

Let’s start with the simple data sex and height.

``` r
y = heights$height
set.seed(2)
test = createDataPartition(y, times = 1, p =0.5, list = FALSE)

train_set = heights %>% slice(-test)
test_set = heights%>% slice(test)
```

In this example, the naive Bayes approach is particularly appropriate.
Because we know that the normal distribution is a very good
approximation of the conditional distributions of height given sex for
both classes, females and males. This implies that we can approximate
the conditional distributions by simply estimating averages and standard
deviations from the data

``` r
params = train_set %>%
  group_by(sex) %>%
  summarize(avg = mean(height), sd = sd(height)) ; params
```

    ## # A tibble: 2 x 3
    ##   sex      avg    sd
    ##   <fct>  <dbl> <dbl>
    ## 1 Female  65.3  3.70
    ## 2 Male    69.3  3.60

The prevalence, which we will denote with pi, which is equal to the
probability of y equals 1, can be estimated from the data as well like
this.

``` r
pi = train_set %>%
  summarize (pi = mean(sex =="Female")) %>%
  .$pi
```

Now we can use our estimates of average and standard deviations to get
the actual rule. We get the conditional distributions, f0 and f1, and
then we use Bayes theorem to compute the naive Bayes estimate of the
conditional probability.

``` r
x = test_set$height 

f.0 = dnorm(x, params$avg[2] , params$sd[2])
f.1 = dnorm(x, params$avg[1] , params$sd[1])

p_hat_bayes = f.1*pi / (f.1 * pi + f.0 * (1 - pi))
```

``` r
test_set %>%
  ggplot (aes(x,p_hat_bayes ))+
  geom_line()
```

![](3.Distance_Knn,_Cross-Validation-_and_Generative-Models_files/figure-gfm/11-1.png)<!-- -->

This estimate of the conditional probability looks a lot like a logistic
regression estimate, as we can see in this graph.

### Quadratic discriminant analysis

QDA is a version of Naive Bayes in which we assume that the conditional
probabilities for the predictors are multivariate normal  
Let’s examine an example with 2 predictors (2,7) and we assume that
their conditional distribution is bivariate normal. This implies that we
need to estimate two averages, two standard deviations, and a
correlation for each case, the 7s and the
2s.

``` r
#We can easily estimate these parameters from the data using this simple code.
params = mnist_27$train %>%
  group_by(y) %>%
  summarize(avg1 = mean(x_1),avg2 = mean(x_2),sd1 = sd(x_1),sd2 = sd(x_2), r = cor(x_1,x_2)) ; params
```

    ## # A tibble: 2 x 6
    ##   y      avg1  avg2    sd1    sd2     r
    ##   <fct> <dbl> <dbl>  <dbl>  <dbl> <dbl>
    ## 1 2     0.129 0.283 0.0702 0.0578 0.401
    ## 2 7     0.234 0.288 0.0719 0.105  0.455

We can also visually demonstrate the approach.

``` r
mnist_27[["train"]]  %>% ggplot(aes(x_1,x_2,col = y))+ 
  geom_point()+
  stat_ellipse()
```

![](3.Distance_Knn,_Cross-Validation-_and_Generative-Models_files/figure-gfm/13-1.png)<!-- -->

Once you’ve estimated these two distributions, this defines an estimate
for the conditional probability of y equals 1 given x1 and x2. We can
the caret package to fit the model and obtain predictors.

``` r
train_qda = train (y~ .,
                   method = "qda" , 
                   data = mnist_27$train)

y_hat = predict(train_qda , mnist_27$test)
confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall["Accuracy"]
```

    ## Accuracy 
    ##     0.82

We see that we obtain a relatively good accuracy of 0.82

#### Summary

Although QDA work well here, it becomes harder to use as a number of
predictors increases. Here we have two predictors and have to compute
four means, four standard deviations, and two correlations. How many
parameters would we have to estimate if instead of two predictors we had
10? The main problem comes from estimating correlations for 10
predictors. With 10, we have 45 correlations for each class.
