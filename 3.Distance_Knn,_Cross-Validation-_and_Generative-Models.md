Distance, Knn, Cross Validation, and Generative Models
================
PK (Kasidit) Ratanavijai
07/13/2019

## Distance

The concept of distance is quite intuitive. For example, when we cluster
animals into subgroups, reptiles, amphibians, mammals, we’re implicitly
defining a **distance** that permits us to say what animals are close to
each other.

Many machine learning techniques rely on being able to define distance
between observations using features or predictors.

We can also compute all the distances between all the observations at
once relatively quickly using the function, dist.

An interesting thing to note is that, if we pick a predictor, a pixel,
we can see which pixels are close, meaning that they are either ink
together or they don’t have ink together by observe distance value

## Knn

K-nearest neighbors(Knn) is similar to bin smoothing. But it is easier
to adapt to multiple dimensions. We first defined the distance between
observations based on the features. Basically, for any point for which
you want to estimate the conditional probability, we look at the
k-nearest points and then take an average of these points. Knn give us
the estimated conditional probability. Larger Ks result in smaller
estimates, while smaller Ks result in more flexible and more wiggly
estimates.
